#!/bin/bash

# Shared cluster's token information
MASTER_IP_ADDRESS="192.170.0.200"
token_path="/vagrant/token_k8s"
ca_cert_hash_path="/vagrant/token_ca_cert_hash"
dashboard_token_path="/vagrant/token_dashboard"

# Generate token to be shared between master and nodes
kubeadm token generate > "${token_path}"

# Init Kubeadm (advertising hte Master's ip@)
kubeadm init --token $(cat "${token_path}") --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=$MASTER_IP_ADDRESS

# Generate discovery token ca cert hash
# To be nonnest, I don't fully understand this line (i.e. how the token is 
# generated by openSSL, but I finally decided to blindly trust it...
# The last part of the command actyually copies the token in the shared 
# directory, making it available to the slaves.
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' > "${ca_cert_hash_path}"

# Enable using the cluster as root
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Enable using the cluster as vagrant
su vagrant -c 'mkdir -p $HOME/.kube'
su vagrant -c 'sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config'
su vagrant -c 'sudo chown $(id -u):$(id -g) $HOME/.kube/config'

# Copy the kubernetes config file on the shared folder to make it available to 
# the host machine, and enable to use the cluster from the host machine.
su vagrant -c 'mkdir -p /vagrant/.kube'
su vagrant -c 'sudo cp -i /etc/kubernetes/admin.conf /vagrant/.kube/config'
su vagrant -c 'sudo chown $(id -u):$(id -g) /vagrant/.kube/config'

# Flannel
# For flannel to work correctly, --pod-network-cidr=10.244.0.0/16 has to be 
# passed to kubeadm init (known problem, as I had to do it manually before)
# For Flannel to be able to use the private interface, as default NIC (in 
# order to prevent issues like the ones encountered in the previous (and 
# manual) setup. See more details at: 
#   https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#default-nic-when-using-flannel-as-the-pod-network-in-vagrant
# curl -o /vagrant/kube-flannel-original.yaml https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
# cp /vagrant/kube-flannel-modified.yaml /vagrant/kube-flannel.yaml
# sed 's#"/opt/bin/flanneld",#"/opt/bin/flanneld", "--iface=eth1",#' -i /vagrant/kube-flannel.yaml
kubectl apply -f /vagrant/kube-flannel.yaml

# Test - deploy the calico network
#curl https://docs.projectcalico.org/v3.11/manifests/calico.yaml -O /vagrant/
#cp /vagrant/calico.yaml /vagrant/calico-original.yaml
#POD_CIDR="10.244.0.0/16"
#sed -i -e "s?192.168.0.0/16?$POD_CIDR?g" /vagrant/calico.yaml
#cp /vagrant/calico.yaml /vagrant/calico-modified.yaml
#kubectl apply -f /vagrant/calico.yaml

# Enable pods scheduling on the Master as well as on the Slaves
kubectl taint nodes --all node-role.kubernetes.io/master-

# Deploy the dashboard (web UI)

# Deploy the kubernetes service
kubectl apply -f /vagrant/dashboard-v200b8-recommended.yaml
# Create the corresponding user and role
kubectl apply -f /vagrant/dashboard-adminuser.yaml
kubectl apply -f /vagrant/dashboard-adminrole.yaml
# Retrieve the token (needed to log in) and copy it to a file
# admin_name=$(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
# kubectl -n kubernetes-dashboard describe secret $admin_name > "${dahsboard_token_path}"
